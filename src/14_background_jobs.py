"""Simple Background Worker Queue for Mail Fetch + KI-Verarbeitung."""

from __future__ import annotations

import importlib
import logging
import threading
import uuid
from dataclasses import dataclass, field
from queue import Empty, Queue
from typing import Any, Dict, Optional
from datetime import datetime, UTC

models = importlib.import_module(".02_models", "src")
mail_fetcher_mod = importlib.import_module(".06_mail_fetcher", "src")
google_oauth = importlib.import_module(".10_google_oauth", "src")
encryption = importlib.import_module(".08_encryption", "src")
processing = importlib.import_module(".12_processing", "src")
ai_client = importlib.import_module(".03_ai_client", "src")
auth = importlib.import_module(".07_auth", "src")
# Phase 17: Semantic Search
from src.semantic_search import generate_embedding_for_email

logger = logging.getLogger(__name__)

# Layer 4 Security: Resource Exhaustion Prevention
MAX_EMAILS_PER_REQUEST = 1000


@dataclass
class FetchJob:
    """
    Job fÃ¼r Mail-Abruf mit AI-Verarbeitung (Phase 2: ServiceToken Pattern).
    
    Security: service_token_id statt plaintext master_key verhindert lange RAM-Speicherung.
    Worker lÃ¤dt DEK erst bei Startup, nutzt sie, und lÃ¶scht sie explizit.
    """
    job_id: str
    user_id: int
    account_id: int
    service_token_id: int
    provider: str
    model: str
    max_mails: int = 50
    sanitize_level: int = 2
    meta: Dict[str, Any] = field(default_factory=dict)
    retry_count: int = 0
    max_retries: int = 2
    retry_delays: list[int] = field(default_factory=lambda: [60, 300])


@dataclass
class BatchReprocessJob:
    """
    Job fÃ¼r Batch-Reprocessing aller Emails (Embedding regeneration).
    Phase 2: ServiceToken Pattern fÃ¼r sichere DEK-Verwaltung.
    """
    job_id: str
    user_id: int
    service_token_id: int
    provider: str
    model: str
    meta: Dict[str, Any] = field(default_factory=dict)
    retry_count: int = 0
    max_retries: int = 2
    retry_delays: list[int] = field(default_factory=lambda: [60, 300])


class BackgroundJobQueue:
    """Kleiner in-memory Job-Queue mit einem Worker-Thread."""

    # Security: Limit queue size to prevent memory exhaustion
    MAX_QUEUE_SIZE = 50

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.queue: Queue = Queue(maxsize=self.MAX_QUEUE_SIZE)  # Generic queue for all job types
        self._stop_event = threading.Event()
        self._worker: Optional[threading.Thread] = None
        self._status: Dict[str, Dict[str, Any]] = {}
        self._status_lock = threading.Lock()
        self._SessionFactory = self._init_session_factory()

    def _init_session_factory(self):
        _, Session = models.init_db(self.db_path)
        return Session

    def ensure_worker(self) -> None:
        if self._worker and self._worker.is_alive():
            return
        self._stop_event.clear()
        self._worker = threading.Thread(
            target=self._run, name="mail-helper-worker", daemon=True
        )
        self._worker.start()
        logger.info("ğŸ§µ Hintergrund-Worker gestartet")

    def stop(self) -> None:
        self._stop_event.set()
        if self._worker and self._worker.is_alive():
            self._worker.join(timeout=2)

    def enqueue_fetch_job(
        self,
        *,
        user_id: int,
        account_id: int,
        master_key: str,
        provider: str,
        model: str,
        max_mails: int,
        sanitize_level: int,
        db_session=None,
        meta: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Phase 2: Enqueue Mail-Fetch Job mit ServiceToken Pattern.
        
        Args:
            user_id: User-ID
            account_id: Mail-Account-ID
            master_key: DEK aus Session (wird hier verschlÃ¼sselt in ServiceToken gespeichert)
            provider: AI-Provider
            model: AI-Model
            max_mails: Max Emails pro Fetch
            sanitize_level: Sanitization-Level
            db_session: SQLAlchemy Session fÃ¼r ServiceToken-Erstellung (optional, wird aus Queue erstellt)
            meta: Additional metadata
            
        Returns:
            str: Job-ID
        """
        if not master_key:
            raise ValueError("master_key ist erforderlich")
        
        if max_mails > MAX_EMAILS_PER_REQUEST:
            raise ValueError(
                f"max_mails darf maximal {MAX_EMAILS_PER_REQUEST} sein (gegeben: {max_mails})"
            )
        
        session = db_session if db_session else self._SessionFactory()
        try:
            token, service_token = auth.ServiceTokenManager.create_token(
                user_id=user_id,
                master_key=master_key,
                session=session,
                days=7
            )
            service_token_id = service_token.id
            logger.info(f"âœ… ServiceToken {service_token_id} fÃ¼r Fetch-Job erstellt (expires: {service_token.expires_at})")
        finally:
            if not db_session:
                session.close()

        job_id = uuid.uuid4().hex
        job = FetchJob(
            job_id=job_id,
            user_id=user_id,
            account_id=account_id,
            service_token_id=service_token_id,
            provider=provider,
            model=model,
            max_mails=max_mails,
            sanitize_level=sanitize_level,
            meta=meta or {},
        )
        try:
            self.queue.put(job, block=False)
        except Exception:
            raise ValueError(
                f"Job-Queue ist voll ({self.MAX_QUEUE_SIZE} Jobs). Bitte warten..."
            )
        self._update_status(
            job_id,
            {
                "state": "queued",
                "user_id": user_id,
                "account_id": account_id,
                "provider": provider,
                "service_token_id": service_token_id,
            },
        )
        self.ensure_worker()
        logger.info(
            "ğŸ“¥ Job %s eingereiht (User %s, Account %s, Token %s)", 
            job_id, user_id, account_id, service_token_id
        )
        return job_id

    def enqueue_batch_reprocess_job(
        self,
        *,
        user_id: int,
        master_key: str,
        provider: str,
        model: str,
        db_session=None,
        meta: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Enqueue Batch-Reprocess Job (alle Emails neu embedden).
        Phase 2: ServiceToken Pattern fÃ¼r sichere DEK-Verwaltung.
        """
        if not master_key:
            raise ValueError("master_key ist erforderlich")

        session = db_session if db_session else self._SessionFactory()
        try:
            token, service_token = auth.ServiceTokenManager.create_token(
                user_id=user_id,
                master_key=master_key,
                session=session,
                days=7
            )
            service_token_id = service_token.id
            logger.info(f"âœ… ServiceToken {service_token_id} fÃ¼r Batch-Reprocess Job erstellt")
        finally:
            if not db_session:
                session.close()

        job_id = uuid.uuid4().hex
        job = BatchReprocessJob(
            job_id=job_id,
            user_id=user_id,
            service_token_id=service_token_id,
            provider=provider,
            model=model,
            meta=meta or {},
        )
        try:
            self.queue.put(job, block=False)
        except Exception:
            raise ValueError(
                f"Job-Queue ist voll ({self.MAX_QUEUE_SIZE} Jobs). Bitte warten..."
            )
        self._update_status(
            job_id,
            {
                "state": "queued",
                "user_id": user_id,
                "provider": provider,
                "model": model,
                "job_type": "batch_reprocess",
                "service_token_id": service_token_id,
            },
        )
        self.ensure_worker()
        logger.info(
            "ğŸ”„ Batch-Reprocess Job %s eingereiht (User %s, Token %s)", job_id, user_id, service_token_id
        )
        return job_id

    def get_status(self, job_id: str, user_id: int) -> Optional[Dict[str, Any]]:
        with self._status_lock:
            status = self._status.get(job_id)
            if not status or status.get("user_id") != user_id:
                return None
            filtered = status.copy()
            filtered.pop("user_id", None)
            return filtered

    def _update_status(self, job_id: str, payload: Dict[str, Any]) -> None:
        with self._status_lock:
            self._status[job_id] = {**self._status.get(job_id, {}), **payload}

    def _run(self) -> None:
        while not self._stop_event.is_set():
            try:
                job = self.queue.get(timeout=1)
            except Empty:
                continue
            try:
                self._update_status(job.job_id, {"state": "running"})
                
                # Route zu spezifischer Execute-Methode basierend auf Job-Typ
                if isinstance(job, BatchReprocessJob):
                    self._execute_batch_reprocess_job(job)
                elif isinstance(job, FetchJob):
                    self._execute_fetch_job(job)
                else:
                    logger.error(f"Unknown job type: {type(job)}")
                    
            except Exception:
                logger.exception("Unerwarteter Fehler im Worker")
            finally:
                self.queue.task_done()

    def _is_transient_error(self, exc: Exception) -> bool:
        """P2-002: Erkennt ob Fehler transient ist (retry lohnt sich)
        
        Transient Errors:
        - Netzwerk-Timeouts
        - IMAP-Server temporÃ¤r nicht erreichbar
        - Datenbank-Locks
        - API-Rate-Limits
        
        Permanent Errors:
        - Falsche Credentials
        - ValueError (falsche Parameter)
        - Permission Denied
        """
        error_str = str(exc).lower()
        error_type = type(exc).__name__
        
        # Transient: Netzwerk-Probleme
        transient_keywords = [
            'timeout', 'timed out', 'connection', 'network',
            'temporary', 'unavailable', 'try again',
            'rate limit', 'too many requests', 'socket',
            'ssl', 'certificate', 'refused'
        ]
        
        if any(keyword in error_str for keyword in transient_keywords):
            return True
        
        # Permanent: Auth/Permission
        permanent_keywords = [
            'authentication', 'credentials', 'password',
            'permission denied', 'unauthorized', 'forbidden',
            'invalid token', 'access denied'
        ]
        
        if any(keyword in error_str for keyword in permanent_keywords):
            return False
        
        # Permanent: Programming Errors
        if error_type in ['ValueError', 'TypeError', 'AttributeError', 'KeyError']:
            return False
        
        # Default: Bei Unsicherheit retry versuchen
        return True

    def _get_dek_from_service_token(self, job, session) -> str:
        """
        Phase 2: LÃ¤dt und verifiziert ServiceToken, gibt DEK zurÃ¼ck.
        
        Sicherheit: 
        - Token wird auf GÃ¼ltigkeit Ã¼berprÃ¼ft
        - last_verified_at wird aktualisiert (Audit-Trail)
        - DEK wird aus DB geladen, nicht aus Job-Dataclass
        
        Args:
            job: FetchJob oder BatchReprocessJob mit service_token_id
            session: DB-Session
            
        Returns:
            str: DEK (Base64-encoded)
            
        Raises:
            ValueError: Token nicht gefunden/abgelaufen
        """
        service_token = session.query(models.ServiceToken).filter_by(
            id=job.service_token_id
        ).first()
        
        if not service_token:
            raise ValueError(f"ServiceToken {job.service_token_id} nicht gefunden")
        
        if not service_token.is_valid():
            raise ValueError(f"ServiceToken {job.service_token_id} abgelaufen (expires: {service_token.expires_at})")
        
        dek = service_token.encrypted_dek
        service_token.mark_verified()
        session.commit()
        
        logger.info(f"âœ… DEK aus ServiceToken {job.service_token_id} geladen")
        return dek

    def _execute_fetch_job(self, job: FetchJob) -> None:
        """
        Execute Mail Fetch Job mit Phase 2 ServiceToken Pattern.
        
        DEK wird hier geladen, in RAM genutzt, dann explizit bereinigt.
        """
        session = self._SessionFactory()
        saved = 0
        processed = 0
        master_key = None
        
        try:
            user = session.query(models.User).filter_by(id=job.user_id).first()
            if not user:
                raise ValueError("User nicht gefunden")

            account = (
                session.query(models.MailAccount)
                .filter_by(id=job.account_id, user_id=job.user_id)
                .first()
            )
            if not account:
                raise ValueError("Mail-Account nicht gefunden")

            master_key = self._get_dek_from_service_token(job, session)
            if not master_key:
                raise ValueError("DEK konnte nicht aus ServiceToken geladen werden")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # SCHRITT 1: State mit Server synchronisieren (DELETE + INSERT pro Ordner)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            import json
            mail_sync_v2 = importlib.import_module(".services.mail_sync_v2", "src")
            
            # Account-spezifische Filter laden
            include_folders = None
            if account.fetch_include_folders:
                try:
                    include_folders = json.loads(account.fetch_include_folders)
                    logger.info(f"ğŸ”„ Schritt 1: State-Sync fÃ¼r Ordner: {include_folders}")
                except json.JSONDecodeError:
                    logger.warning("UngÃ¼ltiges JSON in fetch_include_folders, ignoriere Filter")
            
            # IMAP-Verbindung fÃ¼r State-Sync aufbauen
            MailFetcher = mail_fetcher_mod.MailFetcher
            fetcher = MailFetcher(
                server=encryption.CredentialManager.decrypt_server(
                    account.encrypted_imap_server, master_key
                ),
                username=encryption.CredentialManager.decrypt_email_address(
                    account.encrypted_imap_username, master_key
                ),
                password=encryption.CredentialManager.decrypt_imap_password(
                    account.encrypted_imap_password, master_key
                ),
                port=account.imap_port,
            )
            fetcher.connect()
            
            # Progress-Callback fÃ¼r State-Sync
            def state_sync_progress(phase, message, **kwargs):
                """Callback fÃ¼r kontinuierliche Progress-Updates wÃ¤hrend State-Sync."""
                logger.info(f"ğŸ”” DEBUG: Callback aufgerufen! phase={phase}, message={message}")
                self._update_status(
                    job.job_id,
                    {
                        "phase": phase,
                        "message": message,
                        **kwargs
                    }
                )
                logger.info(f"âœ… DEBUG: Status updated fÃ¼r Job {job.job_id}")
            
            logger.info(f"ğŸ¯ DEBUG: Callback-Funktion definiert, Ã¼bergebe an sync_state_with_server")
            
            try:
                # Schritt 1: Nur State-Sync (kein Raw-Sync hier!)
                sync_service = mail_sync_v2.MailSyncServiceV2(
                    imap_connection=fetcher.connection,
                    db_session=session,
                    user_id=user.id,
                    account_id=account.id
                )
                stats1 = sync_service.sync_state_with_server(
                    include_folders, 
                    progress_callback=state_sync_progress
                )
                
                logger.info(
                    f"âœ… Schritt 1: {stats1.folders_scanned} Ordner, "
                    f"{stats1.mails_on_server} Server-Mails, "
                    f"+{stats1.state_inserted} -{stats1.state_deleted} State"
                )
                
                # Account Last Sync Timestamp aktualisieren
                account.last_server_sync_at = datetime.now(UTC)
                session.commit()
                
            finally:
                fetcher.disconnect()
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # SCHRITT 2: Neue Mails fetchen (Delta-Fetch, UNVERÃ„NDERT)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            self._update_status(
                job.job_id,
                {
                    "phase": "fetch_mails",
                    "message": "Lade neue Mails...",
                }
            )
            
            raw_emails = self._fetch_raw_emails(
                account, master_key, job.max_mails
            )
            
            # Phase 13C Part 3 FINAL: Keine Deduplizierung nÃ¶tig!
            # IMAP UID ist eindeutig pro (account, folder, uid)
            # INBOX/UID=123 â‰  Archiv/UID=123 (verschiedene IMAP-Objekte!)
            # â†’ _persist_raw_emails() macht INSERT/UPDATE per (account, folder, uid)
            
            if raw_emails:
                logger.info(f"ğŸ“§ {len(raw_emails)} Mails abgerufen, speichere in DB...")
                saved = self._persist_raw_emails(
                    session, user, account, raw_emails, master_key
                )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # SCHRITT 3: raw_emails mit State synchronisieren (MOVE-Erkennung!)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            self._update_status(
                job.job_id,
                {
                    "phase": "sync_raw",
                    "message": "Synchronisiere Mails...",
                }
            )
            
            try:
                stats3 = sync_service.sync_raw_emails_with_state()
                logger.info(
                    f"âœ… Schritt 3: {stats3.raw_updated} MOVE erkannt, "
                    f"{stats3.raw_deleted} gelÃ¶scht, {stats3.raw_linked} verlinkt"
                )
            except Exception as e:
                logger.error(f"âŒ Schritt 3 fehlgeschlagen: {e}")

            ai_instance = ai_client.build_client(job.provider, model=job.model)

            def progress_callback(idx: int, total: int, subject: str) -> None:
                self._update_status(
                    job.job_id,
                    {
                        "phase": None,  # âš ï¸ Phase clearen = Frontend zeigt Email-Counter
                        "message": None,
                        "current_email_index": idx,
                        "total_emails": total,
                        "current_subject": subject,
                    },
                )

            processed = processing.process_pending_raw_emails(
                session=session,
                user=user,
                master_key=master_key,
                mail_account=account,
                limit=job.max_mails,
                ai=ai_instance,
                sanitize_level=job.sanitize_level,
                progress_callback=progress_callback,
            )
            
            # Phase Change: Processing fertig, starte Auto-Rules
            self._update_status(
                job.job_id,
                {
                    "phase": "auto_rules",
                    "message": "Wende Auto-Rules an...",
                }
            )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # PHASE G.2: Auto-Action Rules nach Email-Fetch anwenden
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                from src.auto_rules_engine import AutoRulesEngine
                
                rules_engine = AutoRulesEngine(user.id, master_key, session)
                # Verarbeite alle neuen Mails der letzten 60 Minuten
                rules_stats = rules_engine.process_new_emails(
                    since_minutes=60,
                    limit=500
                )
                
                if rules_stats["rules_triggered"] > 0:
                    logger.info(
                        f"ğŸ¤– Auto-Rules: {rules_stats['rules_triggered']} Regeln auf "
                        f"{rules_stats['emails_checked']} E-Mails angewendet"
                    )
            except Exception as rules_err:
                # Auto-Rules sind optional - Job sollte nicht scheitern
                logger.warning(f"âš ï¸ Auto-Rules fehlgeschlagen: {rules_err}")
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            account.last_fetch_at = datetime.now(UTC)
            
            if not account.initial_sync_done:
                account.initial_sync_done = True
                logger.info(f"ğŸ‰ Initialer Sync fÃ¼r Account {account.id} abgeschlossen")
            
            session.commit()

            self._update_status(
                job.job_id,
                {
                    "state": "done",
                    "saved": saved,
                    "processed": processed,
                    "account_id": job.account_id,
                },
            )
            logger.info(
                "âœ… Job %s abgeschlossen (saved=%s, processed=%s)",
                job.job_id,
                saved,
                processed,
            )
        except Exception as exc:
            session.rollback()
            logger.error("âŒ Job %s fehlgeschlagen: %s", job.job_id, exc, exc_info=True)
            
            # P2-002: Retry-Logik mit Exponential Backoff
            should_retry = (
                job.retry_count < job.max_retries and
                self._is_transient_error(exc)
            )
            
            if should_retry:
                job.retry_count += 1
                retry_delay = job.retry_delays[min(job.retry_count - 1, len(job.retry_delays) - 1)]
                
                logger.warning(
                    f"â° Job {job.job_id} wird in {retry_delay}s erneut versucht "
                    f"(Versuch {job.retry_count}/{job.max_retries})"
                )
                
                self._update_status(
                    job.job_id,
                    {
                        "state": "retrying",
                        "message": f"Fehler - Retry in {retry_delay}s (Versuch {job.retry_count}/{job.max_retries})",
                        "retry_count": job.retry_count,
                        "next_retry": datetime.now(UTC).timestamp() + retry_delay,
                    },
                )
                
                # Schedule retry nach delay
                threading.Timer(retry_delay, lambda: self.queue.put(job)).start()
            else:
                # Kein Retry mehr - Job final fehlgeschlagen
                error_msg = "Verarbeitung fehlgeschlagen (siehe Server-Logs)"
                if job.retry_count >= job.max_retries:
                    error_msg += f" - {job.max_retries} Versuche fehlgeschlagen"
                
                self._update_status(
                    job.job_id,
                    {
                        "state": "error",
                        "message": error_msg,
                        "retry_count": job.retry_count,
                    },
                )
        finally:
            # ğŸ”’ Security: Sichere Master-Key Bereinigung aus RAM
            if 'master_key' in locals() and master_key is not None:
                import gc
                master_key = b'\x00' * len(master_key) if isinstance(master_key, bytes) else '\x00' * len(master_key)
                del master_key
                gc.collect()
            session.close()

    def _fetch_raw_emails(
        self, account, master_key: str, limit: int
    ) -> list[Dict[str, Any]]:
        """
        Fetcht Mails aus ALLEN Ordnern (Phase 13C: Multi-Folder FULL SYNC)
        
        Phase 13C Part 3: KEIN UNSEEN-Filter mehr!
        Problem: UNSEEN-Filter fÃ¼hrte zu unvollstÃ¤ndigem Sync (nur 2/20 Mails)
        LÃ¶sung: Server = Single Source of Truth â†’ ALLE Mails fetchen
        
        Phase 13C Part 4: Delta-Sync Option
        - Wenn user.fetch_use_delta_sync=True: Nur neue Mails seit letztem Sync
        - Sonst: FULL SYNC (alle Mails)
        
        Warum? Mails kÃ¶nnen manuell (auÃŸerhalb App) verschoben werden.
        Wir mÃ¼ssen alle Ordner scannen, um DB mit Server zu synchronisieren.
        """
        if account.oauth_provider == "google":
            decrypted_token = encryption.CredentialManager.decrypt_imap_password(
                account.encrypted_oauth_token, master_key
            )
            fetcher = google_oauth.GoogleMailFetcher(access_token=decrypted_token)
            # Google OAuth: Nur INBOX (Gmail nutzt Labels, nicht Folders)
            return fetcher.fetch_new_emails(limit=limit)

        if not account.encrypted_imap_password:
            raise ValueError("Kein IMAP-Passwort gespeichert")

        imap_server = encryption.CredentialManager.decrypt_server(
            account.encrypted_imap_server, master_key
        )
        imap_username = encryption.CredentialManager.decrypt_email_address(
            account.encrypted_imap_username, master_key
        )
        imap_password = encryption.CredentialManager.decrypt_imap_password(
            account.encrypted_imap_password, master_key
        )
        fetcher = mail_fetcher_mod.MailFetcher(
            server=imap_server,
            username=imap_username,
            password=imap_password,
            port=account.imap_port,
        )
        fetcher.connect()
        try:
            # Phase 13C: Fetch aus ALLEN Ordnern, nicht nur INBOX
            all_emails = []
            
            # 1. Liste alle Ordner (IMAPClient: list_folders() gibt direkt Liste zurÃ¼ck!)
            mailboxes = fetcher.connection.list_folders()
            
            # mailboxes ist Liste von (flags, delimiter, name) Tuples
            # Format: [(flags, b'/', 'INBOX'), (flags, b'/', 'Sent'), ...]
            folders = []
            for flags, delimiter, folder_name in mailboxes:
                # IMAPClient gibt Namen schon decoded zurÃ¼ck!
                folders.append(folder_name)
            # mailboxes ist Liste von (flags, delimiter, name) Tuples
            # Format: [(flags, b'/', 'INBOX'), (flags, b'/', 'Sent'), ...]
            folders = []
            for flags, delimiter, folder_name in mailboxes:
                # IMAPClient gibt Namen schon decoded zurÃ¼ck!
                folders.append(folder_name)
            
            # Phase 13C Part 4: User Fetch-Einstellungen laden (global)
            import json
            from datetime import datetime
            
            user_use_delta = getattr(account.user, 'fetch_use_delta_sync', True)
            user_mails_per_folder = getattr(account.user, 'fetch_mails_per_folder', 100)
            
            # Phase 13C Part 6: Account-spezifische Filter
            account_since_date = getattr(account, 'fetch_since_date', None)
            account_unseen_only = getattr(account, 'fetch_unseen_only', False)
            account_include_folders_json = getattr(account, 'fetch_include_folders', None)
            account_exclude_folders_json = getattr(account, 'fetch_exclude_folders', None)
            
            # Parse JSON-Listen
            include_folders = []
            exclude_folders = []
            try:
                if account_include_folders_json:
                    include_folders = json.loads(account_include_folders_json)
                if account_exclude_folders_json:
                    exclude_folders = json.loads(account_exclude_folders_json)
            except json.JSONDecodeError:
                logger.warning("UngÃ¼ltiges JSON in Ordner-Filter, ignoriere")
            
            # Log Fetch-Konfiguration
            filters_active = []
            if account_since_date:
                filters_active.append(f"SINCE {account_since_date}")
            if account_unseen_only:
                filters_active.append("UNSEEN")
            if include_folders:
                filters_active.append(f"Include: {', '.join(include_folders[:3])}{'...' if len(include_folders) > 3 else ''}")
            if exclude_folders:
                filters_active.append(f"Exclude: {', '.join(exclude_folders[:3])}{'...' if len(exclude_folders) > 3 else ''}")
            
            filter_str = f" | {', '.join(filters_active)}" if filters_active else ""
            
            if user_use_delta and account.initial_sync_done:
                logger.info(f"ğŸ“ {len(folders)} Ordner, DELTA SYNC{filter_str}")
            else:
                logger.info(f"ğŸ“ {len(folders)} Ordner, FULL SYNC{filter_str}")
            
            # Phase 13C Part 4: User-steuerbare Limits
            mails_per_folder = max(100, limit // len(folders)) if folders else limit
            mails_per_folder = min(mails_per_folder, user_mails_per_folder)
            
            # Phase 13C Part 4: Delta-Sync: Hole highest UID per folder aus DB
            session = object.__getattribute__(account, "_sa_instance_state").session
            folder_max_uids = {}
            
            if user_use_delta and account.initial_sync_done:
                # Query: SELECT imap_folder, MAX(imap_uid) FROM raw_emails WHERE account_id=X GROUP BY imap_folder
                from sqlalchemy import func
                results = (
                    session.query(
                        models.RawEmail.imap_folder,
                        func.max(func.cast(models.RawEmail.imap_uid, models.Integer)).label('max_uid')
                    )
                    .filter(
                        models.RawEmail.mail_account_id == account.id,
                        models.RawEmail.deleted_at.is_(None)
                    )
                    .group_by(models.RawEmail.imap_folder)
                    .all()
                )
                folder_max_uids = {folder: max_uid for folder, max_uid in results if max_uid}
                logger.info(f"  ğŸ“Š Max UIDs: {folder_max_uids}")
            
            # Part 5: Ordner filtern
            filtered_folders = []
            for folder_name in folders:
                # Ordner-Filter anwenden
                if include_folders and folder_name not in include_folders:
                    logger.debug(f"  â­ï¸  {folder_name}: Nicht in Include-Liste")
                    continue
                
                if exclude_folders and folder_name in exclude_folders:
                    logger.debug(f"  â­ï¸  {folder_name}: In Exclude-Liste")
                    continue
                
                filtered_folders.append(folder_name)
            
            logger.info(f"  ğŸ“‚ {len(filtered_folders)}/{len(folders)} Ordner nach Filter")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # WICHTIG: Lade existierende message_ids auf ACCOUNT-Ebene!
            # Eine Mail in INBOX soll nicht nochmal fÃ¼r Archiv gefetcht werden!
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            existing_msgids_result = (
                session.query(models.RawEmail.message_id)
                .filter(
                    models.RawEmail.mail_account_id == account.id,
                    models.RawEmail.deleted_at.is_(None),
                    models.RawEmail.message_id.isnot(None)
                )
                .all()
            )
            existing_msgids = set(r[0] for r in existing_msgids_result if r[0])
            logger.debug(f"  ğŸ“Š {len(existing_msgids)} message_ids bereits in DB (Account-Ebene)")
            
            for folder_name in filtered_folders:
                try:
                    # Phase 13C Part 6: SINCE-Datum und UNSEEN-Filter vom Account
                    fetch_since = None
                    fetch_unseen = account_unseen_only
                    
                    if account_since_date:
                        fetch_since = datetime.combine(account_since_date, datetime.min.time())
                    
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # KORREKTUR: Filter-basiertes Delta-Sync
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # Wenn Filter aktiv sind (SINCE/UNSEEN), kÃ¶nnen wir nicht einfach
                    # "UID > max_uid" verwenden, da Ã¤ltere Mails dem Filter entsprechen kÃ¶nnten.
                    # Stattdessen: Hole alle UIDs die dem Filter entsprechen und vergleiche mit DB.
                    uid_range = None
                    missing_uids = None
                    
                    if user_use_delta and account.initial_sync_done:
                        # Hole existierende UIDs aus DB fÃ¼r diesen Ordner
                        db_uids_result = (
                            session.query(models.RawEmail.imap_uid)
                            .filter(
                                models.RawEmail.mail_account_id == account.id,
                                models.RawEmail.imap_folder == folder_name,
                                models.RawEmail.deleted_at.is_(None)
                            )
                            .all()
                        )
                        db_uids = set(int(r[0]) for r in db_uids_result if r[0])
                        
                        if fetch_since or fetch_unseen:
                            # Filter-basiertes Delta: Hole UIDs vom Server die dem Filter entsprechen
                            try:
                                server_uids = fetcher.search_uids(
                                    folder=folder_name,
                                    since=fetch_since,
                                    unseen_only=fetch_unseen
                                )
                                if server_uids:
                                    # Berechne echtes Delta
                                    missing_uids = [uid for uid in server_uids if uid not in db_uids]
                                    logger.info(
                                        f"  ğŸ”„ {folder_name}: Filter-Delta {len(missing_uids)}/{len(server_uids)} "
                                        f"(Server: {len(server_uids)}, DB: {len(db_uids)})"
                                    )
                                    if not missing_uids:
                                        logger.info(f"  âœ“ {folder_name}: Alle {len(server_uids)} Mails bereits in DB")
                                        continue
                            except Exception as search_err:
                                logger.warning(f"  âš ï¸ UID-Search fehlgeschlagen fÃ¼r {folder_name}: {search_err}")
                                # Fallback: Ohne Delta-Filter, hole alles was dem Filter entspricht
                                missing_uids = None
                        else:
                            # Kein SINCE/UNSEEN-Filter: Klassisches UID-basiertes Delta
                            if folder_name in folder_max_uids:
                                last_uid = folder_max_uids[folder_name]
                                uid_range = f"{last_uid + 1}:*"
                                logger.info(f"  ğŸ”„ {folder_name}: Delta ab UID {last_uid + 1}")
                    
                    # IMAPClient braucht einfach den folder_name!
                    folder_emails = fetcher.fetch_new_emails(
                        folder=folder_name,
                        limit=mails_per_folder,
                        unseen_only=fetch_unseen if not missing_uids else False,  # Filter schon in UIDs
                        uid_range=uid_range,
                        since=fetch_since if not missing_uids else None,  # Filter schon in UIDs
                        account_id=account.id,
                        session=session,
                        specific_uids=missing_uids  # NEU: Nur diese UIDs laden
                    )
                    all_emails.extend(folder_emails)
                    logger.info(f"  âœ“ {folder_name}: {len(folder_emails)} Mails")
                except Exception as e:
                    logger.warning(f"  âš ï¸ Fehler in Ordner '{folder_name}': {e}")
                    continue
            
            logger.info(f"ğŸ“§ Gesamt: {len(all_emails)} Mails aus {len(folders)} Ordnern")
            
            # Phase 13C Part 4: User-steuerbare Gesamt-Begrenzung
            user_max_total = getattr(account.user, 'fetch_max_total', 0)
            if user_max_total > 0:
                all_emails = all_emails[:user_max_total]
                logger.info(f"  âœ‚ï¸ Auf {user_max_total} Mails begrenzt (user_prefs)")
            
            return all_emails
            
        finally:
            fetcher.disconnect()

    def _persist_raw_emails(
        self, session, user, account, raw_emails: list[Dict[str, Any]], master_key: str
    ) -> int:
        """Speichert RawEmails verschlÃ¼sselt in der Datenbank

        Phase 14e: RFC-konform Unique Key (account, folder, uidvalidity, uid)
        - KEINE Deduplizierungs-Checks mehr!
        - Der Unique Constraint uq_raw_emails_rfc_unique verhindert automatisch Duplikate
        - FÃ¼r jedes Mail: INSERT mit (folder, uidvalidity, uid)
          * Success: Neues Mail gespeichert
          * IntegrityError: Mail existiert bereits (skip)
        - UPDATE nur bei bestehenden Mails (Flags-Ã„nderungen)
        
        Phase 17: Semantic Search - Embedding-Generierung
        - Embedding wird VOR der VerschlÃ¼sselung generiert (Klartext verfÃ¼gbar!)
        - Embeddings sind NICHT verschlÃ¼sselt (nicht reversibel)
        
        Args:
            master_key: Master-Key fÃ¼r VerschlÃ¼sselung (Zero-Knowledge!)
        """
        saved = 0
        skipped = 0
        updated = 0
        
        # Phase 17: AI-Client fÃ¼r Embeddings (User Settings EMBEDDING Model!)
        embedding_ai_client = None
        try:
            # WICHTIG: Nutze EMBEDDING Settings (nicht BASE!)
            provider_embedding = (user.preferred_embedding_provider or "ollama").lower()
            model_embedding = user.preferred_embedding_model or "all-minilm:22m"
            resolved_model = ai_client.resolve_model(provider_embedding, model_embedding)
            
            embedding_ai_client = ai_client.build_client(provider_embedding, model=resolved_model)
            logger.debug(f"âœ… Embedding AI-Client ({resolved_model}) initialisiert")
        except Exception as e:
            logger.warning(f"âš ï¸ Embedding AI-Client nicht verfÃ¼gbar: {e}")
        
        for raw_email_data in raw_emails:
            # Phase 14e: RFC-konform Unique Key
            imap_folder = raw_email_data.get("imap_folder")
            imap_uid = raw_email_data.get("imap_uid")
            imap_uidvalidity = raw_email_data.get("imap_uidvalidity")
            message_id = raw_email_data.get("message_id")
            
            if not imap_folder or not imap_uid or not imap_uidvalidity:
                logger.warning(
                    f"âš ï¸ Mail ohne folder/uid/uidvalidity: "
                    f"{raw_email_data.get('subject', 'N/A')[:30]}"
                )
                continue
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # SCHRITT 2: NUR INSERT - KEINE MOVE-LOGIK HIER!
            # MOVE-Erkennung macht Schritt 3 (sync_raw_emails_with_state)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # Check if exists by folder/uid/uidvalidity (RFC-konform)
            existing = (
                session.query(models.RawEmail)
                .filter_by(
                    user_id=user.id,
                    mail_account_id=account.id,
                    imap_folder=imap_folder,
                    imap_uidvalidity=imap_uidvalidity,
                    imap_uid=imap_uid,
                )
                .filter(models.RawEmail.deleted_at.is_(None))
                .first()
            )
            
            if existing:
                # UPDATE: Mail existiert bereits (gleicher folder/uid), nur Flags aktualisieren
                existing.imap_flags = raw_email_data.get("imap_flags")
                existing.imap_is_seen = raw_email_data.get("imap_is_seen", False)
                existing.imap_is_flagged = raw_email_data.get("imap_is_flagged", False)
                existing.imap_is_answered = raw_email_data.get("imap_is_answered", False)
                existing.imap_last_seen_at = datetime.now(UTC)
                updated += 1
                logger.debug(f"ğŸ”„ UPDATE: {imap_folder}/{imap_uid}")
                continue
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # WICHTIG: PrÃ¼fe ob message_id schon existiert (in IRGENDEINEM Ordner!)
            # Fallback: content_hash (fÃ¼r Mails ohne message_id)
            # 
            # ACHTUNG: Auch soft-deleted Mails prÃ¼fen!
            # â†’ Falls soft-deleted: WIEDERHERSTELLEN (deleted_at=NULL) statt Ã¼berspringen
            # â†’ Falls aktiv: Schritt 3 macht MOVE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            existing_by_msgid = None
            
            # PrimÃ¤r: message_id (inkl. soft-deleted!)
            if message_id:
                existing_by_msgid = (
                    session.query(models.RawEmail)
                    .filter_by(
                        user_id=user.id,
                        mail_account_id=account.id,
                        message_id=message_id,
                    )
                    .first()  # KEIN .filter(deleted_at.is_(None)) - auch gelÃ¶schte finden!
                )
                
                if existing_by_msgid:
                    if existing_by_msgid.deleted_at:
                        # WIEDERHERSTELLEN: Mail war gelÃ¶scht, ist aber wieder auf Server!
                        existing_by_msgid.deleted_at = None
                        existing_by_msgid.imap_folder = imap_folder
                        existing_by_msgid.imap_uid = imap_uid
                        existing_by_msgid.imap_uidvalidity = imap_uidvalidity
                        existing_by_msgid.imap_flags = raw_email_data.get("imap_flags")
                        existing_by_msgid.imap_is_seen = raw_email_data.get("imap_is_seen", False)
                        existing_by_msgid.imap_is_flagged = raw_email_data.get("imap_is_flagged", False)
                        existing_by_msgid.imap_is_answered = raw_email_data.get("imap_is_answered", False)
                        existing_by_msgid.imap_last_seen_at = datetime.now(UTC)
                        updated += 1
                        logger.info(f"â™»ï¸ UNDELETE: {imap_folder}/{imap_uid} (id={existing_by_msgid.id})")
                        continue
                    else:
                        # Mail aktiv, anderer Ordner â†’ Schritt 3 macht MOVE
                        skipped += 1
                        logger.debug(
                            f"â­ï¸ Skip: message_id existiert bereits in {existing_by_msgid.imap_folder} "
                            f"(Schritt 3 macht MOVE nach {imap_folder})"
                        )
                        continue
            
            # Fallback: content_hash (fÃ¼r Mails ohne message_id)
            content_hash = raw_email_data.get("content_hash")
            if not existing_by_msgid and content_hash:
                existing_by_hash = (
                    session.query(models.RawEmail)
                    .filter_by(
                        user_id=user.id,
                        mail_account_id=account.id,
                        content_hash=content_hash,
                    )
                    .first()  # KEIN .filter(deleted_at.is_(None))
                )
                
                if existing_by_hash:
                    if existing_by_hash.deleted_at:
                        # WIEDERHERSTELLEN
                        existing_by_hash.deleted_at = None
                        existing_by_hash.imap_folder = imap_folder
                        existing_by_hash.imap_uid = imap_uid
                        existing_by_hash.imap_uidvalidity = imap_uidvalidity
                        existing_by_hash.imap_flags = raw_email_data.get("imap_flags")
                        existing_by_hash.imap_is_seen = raw_email_data.get("imap_is_seen", False)
                        existing_by_hash.imap_is_flagged = raw_email_data.get("imap_is_flagged", False)
                        existing_by_hash.imap_is_answered = raw_email_data.get("imap_is_answered", False)
                        existing_by_hash.imap_last_seen_at = datetime.now(UTC)
                        updated += 1
                        logger.info(f"â™»ï¸ UNDELETE: {imap_folder}/{imap_uid} (id={existing_by_hash.id})")
                        continue
                    else:
                        skipped += 1
                        logger.debug(
                            f"â­ï¸ Skip: content_hash existiert bereits in {existing_by_hash.imap_folder} "
                            f"(Schritt 3 macht MOVE nach {imap_folder})"
                        )
                        continue
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # PHASE 17: KLARTEXT IST HIER NOCH VERFÃœGBAR!
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            subject_plain = raw_email_data.get("subject", "")
            body_plain = raw_email_data.get("body", "")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # NEU: Embedding generieren (VOR VerschlÃ¼sselung!)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            embedding_bytes = None
            embedding_model = None
            embedding_generated_at = None
            
            if embedding_ai_client and (subject_plain or body_plain):
                try:
                    embedding_bytes, embedding_model, embedding_generated_at = \
                        generate_embedding_for_email(
                            subject=subject_plain,
                            body=body_plain,
                            ai_client=embedding_ai_client,
                            model_name=resolved_model  # Ãœbergebe explizit das Model
                        )
                    if embedding_bytes:
                        logger.debug(f"ğŸ” Embedding generiert fÃ¼r: {subject_plain[:50]}...")
                except Exception as e:
                    logger.warning(f"âš ï¸ Embedding-Fehler: {e}")
            

            encrypted_sender = encryption.EmailDataManager.encrypt_email_sender(
                raw_email_data["sender"], master_key
            )
            encrypted_subject = encryption.EmailDataManager.encrypt_email_subject(
                raw_email_data["subject"], master_key
            )
            encrypted_body = encryption.EmailDataManager.encrypt_email_body(
                raw_email_data["body"], master_key
            )

            encrypted_in_reply_to = None
            if raw_email_data.get("in_reply_to"):
                encrypted_in_reply_to = encryption.EncryptionManager.encrypt_data(
                    raw_email_data["in_reply_to"], master_key
                )

            encrypted_to = None
            if raw_email_data.get("to"):
                encrypted_to = encryption.EncryptionManager.encrypt_data(
                    raw_email_data["to"], master_key
                )

            encrypted_cc = None
            if raw_email_data.get("cc"):
                encrypted_cc = encryption.EncryptionManager.encrypt_data(
                    raw_email_data["cc"], master_key
                )

            encrypted_bcc = None
            if raw_email_data.get("bcc"):
                encrypted_bcc = encryption.EncryptionManager.encrypt_data(
                    raw_email_data["bcc"], master_key
                )

            encrypted_reply_to = None
            if raw_email_data.get("reply_to"):
                encrypted_reply_to = encryption.EncryptionManager.encrypt_data(
                    raw_email_data["reply_to"], master_key
                )

            encrypted_references = None
            if raw_email_data.get("references"):
                encrypted_references = encryption.EncryptionManager.encrypt_data(
                    raw_email_data["references"], master_key
                )

            raw_email = models.RawEmail(
                user_id=user.id,
                mail_account_id=account.id,
                encrypted_sender=encrypted_sender,
                encrypted_subject=encrypted_subject,
                encrypted_body=encrypted_body,
                received_at=raw_email_data["received_at"],
                imap_uid=raw_email_data.get("imap_uid"),
                imap_folder=raw_email_data.get("imap_folder"),
                imap_uidvalidity=raw_email_data.get("imap_uidvalidity"),  # Phase 14e
                imap_flags=raw_email_data.get("imap_flags"),
                message_id=raw_email_data.get("message_id"),
                encrypted_in_reply_to=encrypted_in_reply_to,
                parent_uid=raw_email_data.get("parent_uid"),
                thread_id=raw_email_data.get("thread_id"),
                imap_is_seen=raw_email_data.get("imap_is_seen"),
                imap_is_answered=raw_email_data.get("imap_is_answered"),
                imap_is_flagged=raw_email_data.get("imap_is_flagged"),
                imap_is_deleted=raw_email_data.get("imap_is_deleted"),
                imap_is_draft=raw_email_data.get("imap_is_draft"),
                encrypted_to=encrypted_to,
                encrypted_cc=encrypted_cc,
                encrypted_bcc=encrypted_bcc,
                encrypted_reply_to=encrypted_reply_to,
                message_size=raw_email_data.get("message_size"),
                encrypted_references=encrypted_references,
                content_type=raw_email_data.get("content_type"),
                charset=raw_email_data.get("charset"),
                has_attachments=raw_email_data.get("has_attachments"),
                # Phase 17: Semantic Search - Embeddings (NICHT verschlÃ¼sselt!)
                email_embedding=embedding_bytes,
                embedding_model=embedding_model,
                embedding_generated_at=embedding_generated_at,
            )
            try:
                session.add(raw_email)
                session.flush()  # Force insert to trigger IntegrityError if duplicate
                saved += 1
            except Exception as e:
                session.rollback()
                # Phase 14e: IntegrityError = Duplikat (Unique Constraint)
                if "UNIQUE constraint failed" in str(e) or "IntegrityError" in str(type(e).__name__):
                    skipped += 1
                    logger.debug(
                        f"â­ï¸  Duplikat Ã¼bersprungen: {imap_folder}/{imap_uid} "
                        f"(UIDVALIDITY={imap_uidvalidity})"
                    )
                else:
                    logger.error(f"âŒ Fehler beim Speichern: {e}")
                    raise

        if saved or updated or skipped:
            session.commit()
            if updated > 0 or skipped > 0:
                logger.info(
                    f"ğŸ’¾ {saved} neue Mails, {updated} aktualisiert, "
                    f"{skipped} Duplikate Ã¼bersprungen"
                )
            else:
                logger.info(f"ğŸ’¾ {saved} neue Mails gespeichert")
        else:
            session.flush()

        return saved

    def _execute_batch_reprocess_job(self, job: BatchReprocessJob) -> None:
        """
        Execute Batch-Reprocess Job (regenerate embeddings for all emails).
        Phase 2: ServiceToken Pattern fÃ¼r sichere DEK-Verwaltung.
        """
        session = self._SessionFactory()
        processed = 0
        failed = 0
        master_key = None
        
        try:
            user = session.query(models.User).filter_by(id=job.user_id).first()
            if not user:
                raise ValueError("User nicht gefunden")

            master_key = self._get_dek_from_service_token(job, session)
            if not master_key:
                raise ValueError("DEK konnte nicht aus ServiceToken geladen werden")

            # Hole alle RawEmails des Users
            raw_emails = (
                session.query(models.RawEmail)
                .filter(
                    models.RawEmail.user_id == user.id,
                    models.RawEmail.deleted_at == None
                )
                .all()
            )
            
            if not raw_emails:
                self._update_status(
                    job.job_id,
                    {
                        "state": "done",
                        "processed": 0,
                        "failed": 0,
                        "total_emails": 0,
                        "message": "Keine Emails vorhanden"
                    },
                )
                return
            
            total = len(raw_emails)
            logger.info(f"ğŸ”„ Batch-Reprocess: {total} Emails mit {job.model}")
            
            # Build AI Client
            resolved_model = ai_client.resolve_model(job.provider, job.model)
            embedding_client = ai_client.build_client(job.provider, model=resolved_model)
            
            # Update initial status
            self._update_status(
                job.job_id,
                {
                    "total_emails": total,
                    "current_email_index": 0,
                },
            )
            
            # Process each email
            for idx, raw_email in enumerate(raw_emails, start=1):
                try:
                    # EntschlÃ¼sseln
                    decrypted_subject = encryption.EmailDataManager.decrypt_email_subject(
                        raw_email.encrypted_subject or "", master_key
                    )
                    decrypted_body = encryption.EmailDataManager.decrypt_email_body(
                        raw_email.encrypted_body or "", master_key
                    )
                    
                    # Progress-Update
                    self._update_status(
                        job.job_id,
                        {
                            "current_email_index": idx,
                            "current_subject": decrypted_subject[:50] if decrypted_subject else "Kein Betreff",
                        },
                    )
                    
                    logger.info(f"ğŸ”„ [{idx}/{total}] Verarbeite: {decrypted_subject[:50] if decrypted_subject else 'Kein Betreff'}...")
                    
                    # Embedding generieren
                    embedding_bytes, model_name, timestamp = generate_embedding_for_email(
                        subject=decrypted_subject,
                        body=decrypted_body,
                        ai_client=embedding_client,
                        model_name=resolved_model  # Ãœbergebe explizit das Model
                    )
                    
                    if embedding_bytes:
                        old_embedding = raw_email.email_embedding
                        raw_email.email_embedding = embedding_bytes
                        raw_email.embedding_model = model_name or resolved_model
                        raw_email.embedding_generated_at = timestamp
                        
                        # Commit nach jedem Email fÃ¼r bessere Fehlertoleranz
                        session.flush()
                        
                        processed += 1
                        logger.info(f"âœ… [{idx}/{total}] Email {raw_email.id} embedded ({len(embedding_bytes)} bytes, Model: {model_name or resolved_model})")
                    else:
                        failed += 1
                        logger.warning(f"âš ï¸  [{idx}/{total}] Embedding failed for email {raw_email.id}")
                        
                except Exception as e:
                    failed += 1
                    logger.error(f"âŒ [{idx}/{total}] Failed to reprocess email {raw_email.id}: {e}")
            
            session.commit()
            
            # Tag-Embedding-Cache leeren nach Batch-Reprocess!
            # Wichtig: Tags mÃ¼ssen mit neuem Model re-embedded werden
            try:
                from src.services.tag_manager import TagEmbeddingCache
                TagEmbeddingCache._cache.clear()
                TagEmbeddingCache._ai_client_cache.clear()
                logger.info("ğŸ—‘ï¸  Tag-Embedding-Cache geleert (Tags werden mit neuem Model re-embedded)")
            except Exception as cache_err:
                logger.warning(f"âš ï¸  Konnte Tag-Cache nicht leeren: {cache_err}")
            
            self._update_status(
                job.job_id,
                {
                    "state": "done",
                    "processed": processed,
                    "failed": failed,
                    "total_emails": total,
                    "model": resolved_model,
                },
            )
            logger.info(
                "âœ… Batch-Reprocess Job %s abgeschlossen (processed=%s, failed=%s)",
                job.job_id,
                processed,
                failed,
            )
            
        except Exception as exc:
            session.rollback()
            logger.error("âŒ Batch-Reprocess Job %s fehlgeschlagen: %s", job.job_id, exc, exc_info=True)
            
            # P2-002: Retry-Logik auch fÃ¼r Batch-Jobs
            should_retry = (
                job.retry_count < job.max_retries and
                self._is_transient_error(exc)
            )
            
            if should_retry:
                job.retry_count += 1
                retry_delay = job.retry_delays[min(job.retry_count - 1, len(job.retry_delays) - 1)]
                
                logger.warning(
                    f"â° Batch-Job {job.job_id} wird in {retry_delay}s erneut versucht "
                    f"(Versuch {job.retry_count}/{job.max_retries})"
                )
                
                self._update_status(
                    job.job_id,
                    {
                        "state": "retrying",
                        "message": f"Fehler - Retry in {retry_delay}s (Versuch {job.retry_count}/{job.max_retries})",
                        "retry_count": job.retry_count,
                        "next_retry": datetime.now(UTC).timestamp() + retry_delay,
                    },
                )
                
                # Schedule retry
                threading.Timer(retry_delay, lambda: self.queue.put(job)).start()
            else:
                # Kein Retry mehr
                error_msg = f"Batch-Reprocess fehlgeschlagen: {str(exc)}"
                if job.retry_count >= job.max_retries:
                    error_msg += f" - {job.max_retries} Versuche fehlgeschlagen"
                
                self._update_status(
                    job.job_id,
                    {
                        "state": "error",
                        "message": error_msg,
                        "retry_count": job.retry_count,
                    },
                )
        finally:
            # ğŸ”’ Security: Sichere Master-Key Bereinigung aus RAM
            if 'master_key' in locals() and master_key is not None:
                import gc
                master_key = b'\x00' * len(master_key) if isinstance(master_key, bytes) else '\x00' * len(master_key)
                del master_key
                gc.collect()
            session.close()
