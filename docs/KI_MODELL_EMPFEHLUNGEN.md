# KI-Modell-Empfehlungen f√ºr KI-Mail-Helper

> **Zielgruppe:** Anwender und Administratoren  
> **Stand:** Januar 2026  
> **Version:** 2.0 (Multi-User Edition)  
> **Getestet mit:** Ollama, OpenAI, Anthropic, Mistral

---

## TL;DR: Hardware bestimmt Modell-Wahl

**Mit dedizierter GPU (CUDA):**
- ‚úÖ Gro√üe Modelle nutzbar (llama3.1:8b, mistral:7b)
- ‚úÖ Schnelle Verarbeitung (<1 Sek pro Email)
- ‚úÖ Beste Analyse-Qualit√§t out-of-the-box

**Nur CPU-Betrieb:**
- ‚ö†Ô∏è Kleine Modelle empfohlen (llama3.2:1b, gemma2:2b)
- ‚ö†Ô∏è Langsame Verarbeitung (5-10 Min pro Email)
- ‚úÖ **Learning-System gleicht Qualit√§t aus!**

**üí° Entscheidender Faktor:** Bei CPU-only sind 3-5 manuell getaggte Emails pro Tag wichtiger als ein gr√∂√üeres Modell. Das Learning-System (inkl. Negative Feedback seit 2026-01-06) kompensiert schw√§chere Modelle perfekt.

---

## √úbersicht: Die 3-Stufen-Architektur

KI-Mail-Helper verwendet drei separate KI-Modelle f√ºr unterschiedliche Aufgaben:

| Stufe | Zweck | Wann ausgef√ºhrt |
|-------|-------|-----------------|
| **Embedding** | Semantische Vektoren f√ºr Suche & Tag-Matching | Bei jedem Email-Fetch |
| **Base** | Schnelle Klassifikation (Score, Farbe, Tags) | Bei jedem Email-Fetch |
| **Optimize** | Tiefe Analyse (bessere Tags, Kontext) | Manuell pro Email |

---

## 1. Embedding-Modell

### Aufgabe
Wandelt Email-Text in einen numerischen Vektor (z.B. 384 Dimensionen) um. Dieser Vektor erm√∂glicht:
- **Semantische Suche:** "Finde Emails √ºber Vertragsk√ºndigungen"
- **Tag-Suggestions:** √Ñhnlichkeit zwischen Email und Tag-Beschreibung
- **Learned Tags:** Durchschnitt aller Email-Vektoren eines Tags

### Modell-Vergleich

| Modell | Provider | Dimensionen | Geschwindigkeit | Qualit√§t | Gr√∂√üe |
|--------|----------|-------------|-----------------|----------|-------|
| `all-minilm:22m` | Ollama | 384 | ‚ö°‚ö°‚ö° Sehr schnell | ‚≠ê‚≠ê‚≠ê Gut | 46 MB |
| `nomic-embed-text` | Ollama | 768 | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê Sehr gut | 274 MB |
| `mxbai-embed-large` | Ollama | 1024 | ‚ö° Mittel | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent | 669 MB |
| `bge-m3` | Ollama | 1024 | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent | 567 MB |
| `text-embedding-3-small` | OpenAI | 1536 | ‚ö°‚ö°‚ö° Sehr schnell | ‚≠ê‚≠ê‚≠ê‚≠ê Sehr gut | Cloud |
| `text-embedding-3-large` | OpenAI | 3072 | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent | Cloud |

### Empfehlung: `all-minilm:22m` ‚úÖ

**Begr√ºndung:**

Die Wahl des Embedding-Modells hat **weniger Einfluss auf die Qualit√§t** als urspr√ºnglich angenommen. Unsere Tests zeigten:

| Szenario | all-minilm:22m | mxbai-embed-large |
|----------|----------------|-------------------|
| Description-basierte Tags | 15-25% Similarity | 20-30% Similarity |
| **Learned Tags** (3+ Emails) | **85-95% Similarity** | **87-96% Similarity** |
| **Mit Negative Feedback** | **90-98% Similarity** | **92-99% Similarity** |

**Der Qualit√§tssprung kommt vom Learning, nicht vom Modell.**

Wenn ein Tag aus 3+ manuell zugewiesenen Emails lernt, erreichen beide Modelle exzellente Ergebnisse. Das gr√∂√üere Modell bringt nur ~2-5% mehr Similarity ‚Äì bei 10x l√§ngerer Verarbeitungszeit.

**Seit Phase F.3 (2026-01-06):** Negative Feedback verbessert die Qualit√§t nochmal deutlich:
- System lernt von abgelehnten Tag-Vorschl√§gen
- Penalty-System reduziert false-positives um 40-60%
- Selbst schwache Modelle erreichen nach 5-10 Rejects pro Tag >95% Pr√§zision

**Fazit:** `all-minilm:22m` bietet das beste Verh√§ltnis aus Geschwindigkeit und Qualit√§t. Die investierte Zeit ist bei gr√∂√üeren Modellen besser in manuelles Tagging + Rejecting investiert.

---

## 2. Base-Modell

### Aufgabe
Analysiert jede Email beim Fetch und bestimmt:
- **Score** (1-10): Priorit√§t der Email
- **Farbe** (Rot/Gelb/Gr√ºn): Visuelle Dringlichkeit
- **Dringlichkeit** (1-3): Zeitliche Komponente
- **Wichtigkeit** (1-3): Inhaltliche Relevanz
- **Kategorie:** "aktion_erforderlich" / "dringend" / "nur_information"
- **Tag-Vorschl√§ge:** 1-5 semantische Tags

### Modell-Vergleich

| Modell | Provider | Parameter | Geschwindigkeit | Qualit√§t | VRAM |
|--------|----------|-----------|-----------------|----------|------|
| `llama3.2:1b` | Ollama | 1B | ‚ö°‚ö°‚ö° Sehr schnell | ‚≠ê‚≠ê‚≠ê Gut | 2-3 GB |
| `llama3.2:3b` | Ollama | 3B | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê Sehr gut | 4-5 GB |
| `gemma2:2b` | Ollama | 2B | ‚ö°‚ö°‚ö° Sehr schnell | ‚≠ê‚≠ê‚≠ê Gut | 3 GB |
| `phi3:3.8b` | Ollama | 3.8B | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê Sehr gut | 4 GB |
| `gpt-4o-mini` | OpenAI | - | ‚ö°‚ö°‚ö° Sehr schnell | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent | Cloud |
| `claude-3-5-haiku` | Anthropic | - | ‚ö°‚ö°‚ö° Sehr schnell | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent | Cloud |

### Empfehlung: `llama3.2:1b` (lokal) oder `gpt-4o-mini` (Cloud) ‚úÖ

**Begr√ºndung:**

Das Base-Modell wird bei **jedem Email-Fetch** ausgef√ºhrt. Bei 50 neuen Emails bedeutet das 50 Analysen. Geschwindigkeit ist hier kritisch.

#### Mit dedizierter GPU (CUDA):

| Aspekt | llama3.2:1b | llama3.2:3b | llama3.1:8b |
|--------|-------------|-------------|-------------|
| Zeit pro Email | <1 Sek | ~2 Sek | ~5 Sek |
| Score-Genauigkeit | 85% | 90% | 93% |
| Tag-Qualit√§t | Gut | Sehr gut | Exzellent |

Mit GPU kannst du problemlos gr√∂√üere Modelle nutzen!

#### Nur CPU-Betrieb:

| Aspekt | llama3.2:1b | llama3.2:3b |
|--------|-------------|-------------|
| Zeit pro Email | ~5-8 Min | ~15+ Min |
| Score-Genauigkeit | 85% | 90% |
| Tag-Qualit√§t | Gut | Sehr gut |

**Der Unterschied in der Analyse-Qualit√§t ist marginal**, da:
1. Die KI nur eine Ersteinsch√§tzung liefert
2. User die Bewertung korrigieren k√∂nnen (Learning)
3. Tags √ºber Embeddings + Learning gematcht werden, nicht √ºber das Base-Modell
4. **Negative Feedback** verbessert Tag-Qualit√§t unabh√§ngig vom Modell

**üí° CPU-Only Strategie:**
- Nutze `llama3.2:1b` f√ºr Geschwindigkeit
- Investiere Zeit in manuelles Tagging (3-5 Emails pro Tag)
- Lehne unpassende Vorschl√§ge mit √ó Button ab
- Nach 1-2 Wochen: System kennt deine Pr√§ferenzen besser als ein 8B-Modell

**F√ºr Cloud-User:** `gpt-4o-mini` bietet exzellente Qualit√§t bei sehr niedrigen Kosten (~$0.15 pro 1M Tokens).

---

## 3. Optimize-Modell

### Aufgabe
F√ºhrt eine tiefere Analyse durch, wenn der User auf "Optimieren" klickt:
- Bessere Tag-Vorschl√§ge durch l√§ngere Analyse
- Kontext aus Email-Thread ber√ºcksichtigen
- Feinere Unterscheidung bei ambivalenten Emails

### Modell-Vergleich

| Modell | Provider | Parameter | Geschwindigkeit | Qualit√§t | Kosten |
|--------|----------|-----------|-----------------|----------|--------|
| `llama3.2:3b` | Ollama | 3B | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê Sehr gut | Kostenlos |
| `llama3.1:8b` | Ollama | 8B | ‚ö° Langsam | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent | Kostenlos |
| `gpt-4o` | OpenAI | - | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent | ~$5/1M |
| `claude-3-5-sonnet` | Anthropic | - | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent | ~$3/1M |
| `mistral-large` | Mistral | - | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent | ~$2/1M |

### Empfehlung: `llama3.2:3b` (lokal) oder `claude-3-5-sonnet` (Cloud) ‚úÖ

**Begr√ºndung:**

Optimize wird nur **manuell und selten** ausgef√ºhrt. Hier lohnt sich ein gr√∂√üeres Modell, da:
1. Geschwindigkeit weniger kritisch ist
2. Komplexe Emails von besserer Analyse profitieren
3. Der User aktiv auf das Ergebnis wartet

**F√ºr rein lokalen Betrieb:** `llama3.2:3b` bietet gute Qualit√§t ohne Cloud-Abh√§ngigkeit.

**F√ºr beste Ergebnisse:** Cloud-Modelle wie `claude-3-5-sonnet` oder `gpt-4o` liefern die pr√§zisesten Analysen.

---

## Finale Empfehlung

### Lokaler Betrieb (Privacy-First)

| Stufe | Modell | Begr√ºndung |
|-------|--------|------------|
| **Embedding** | `all-minilm:22m` | Schnell, Qualit√§t kommt vom Learning |
| **Base** | `llama3.2:1b` | Beste Balance Geschwindigkeit/Qualit√§t |
| **Optimize** | `llama3.2:3b` | Tiefere Analyse bei manueller Nutzung |

**Ollama-Installation:**
```bash
ollama pull all-minilm:22m
ollama pull llama3.2:1b
ollama pull llama3.2:3b
```

**Ressourcen-Bedarf:**
- RAM: 8 GB minimum, 16 GB empfohlen
- Speicher: ~3.5 GB f√ºr alle Modelle
- GPU: Optional, aber 10-50x schneller

---

### Hybrid-Betrieb (Lokal + Cloud)

| Stufe | Modell | Begr√ºndung |
|-------|--------|------------|
| **Embedding** | `all-minilm:22m` | Lokal, keine Daten in Cloud |
| **Base** | `gpt-4o-mini` | Schnell, g√ºnstig, exzellent |
| **Optimize** | `claude-3-5-sonnet` | Beste Qualit√§t f√ºr wichtige Emails |

**Vorteile:**
- Embeddings bleiben lokal (Privacy) + Negative Feedback

> **Wichtig:** Die Wahl des Modells ist weniger entscheidend als das **Tag-Learning-System** (seit Phase F.2/F.3).

### Wie Learning funktioniert

**Positive Learning (seit Phase F.2):**
1. User weist Tag manuell zu einer Email zu
2. System speichert Email-Embedding als "positives Beispiel"
3. Nach 3+ Beispielen: Tag bekommt "Learned Embedding" (Durchschnitt)
4. Neue Emails werden gegen Learned Embedding gematcht
5. Similarity steigt von ~20% (Description) auf ~90% (Learned)

**Negative Learning (seit Phase F.3 - 2026-01-06):**
1. User lehnt unpassenden Tag-Vorschlag mit √ó Button ab
2. System speichert Email-Embedding als "negatives Beispiel"
3. N√§chste Email: Penalty wird vom Similarity-Score abgezogen
4. Je mehr Rejects (Count-Bonus), desto st√§rker die Penalty
5. False-positive Rate sinkt um 40-60% nach ~5-10 Rejects pro Tag

### Warum Learning wichtiger ist als Modellgr√∂√üe

| Ansatz | Similarity | Zeitaufwand | Hardware-Anforderung |
|--------|------------|-------------|---------------------|
| Gr√∂√üeres Embedding-Modell | +5% | +500% Rechenzeit | Mehr RAM/GPU |
| 3 Emails manuell taggen | +60% | 30 Sekunden | Keine |
| 5 Rejects pro Tag | +40% Pr√§zision | 10 Sekunden | Keine |

**Besonders f√ºr CPU-only Systeme:**

Ein **llama3.2:1b + Learning** schl√§gt ein **llama3.1:8b ohne Learning**:

| Szenario | llama3.2:1b + Learning | llama3.1:8b ohne Learning |
|----------|------------------------|---------------------------|
| Tag-Vorschl√§ge Pr√§zision | 92-95% | 85-88% |
| False-Positives | 5-8% | 12-15% |
| Verarbeitungszeit (50 Emails) | ~5-8 Stunden | ~40+ Stunden |

**Fazit:** 
- CPU-only ‚Üí Investiere Zeit ins Tagging, nicht in gr√∂√üere Modelle
- GPU verf√ºgbar ‚Üí Gr√∂√üere Modelle sind OK, aber Learning bleibt kritisch
- **Negative Feedback ist ein Game-Changer** f√ºr schwache Hardware
> **Wichtig:** Die Wahl des Modells ist weniger entscheidend als das **Tag-Learning-System**.
dedizierter GPU (RTX 3060, RTX 4060 oder besser, CUDA Support)

| Operation | Zeit (1B Modell) | Zeit (8B Modell) |
|-----------|------------------|------------------|
| Email-Embedding | ~50 ms | ~50 ms |
| Base-Analyse | <1 Sekunde | ~5 Sekunden |
| Optimize-Analyse | ~2-3 Sekunden | ~10-15 Sekunden |
| **50 Emails verarbeiten** | **~2-3 Minuten** | **~10-15 Minuten** |

**GPU-Empfehlung:**
- RTX 3060 (12 GB): llama3.1:8b problemlos nutzbar
- RTX 4090 (24 GB): llama3.1:70b m√∂glich (Optimize)
- Server mit A100: Beliebige Modellgr√∂√üen

### Nur CPU (Intel i5/i7, AMD Ryzen, ohne dedizierte GPU)

| Operation | Zeit (1B Modell) | Zeit (3B Modell) | Zeit (8B Modell) |
|-----------|------------------|------------------|------------------|
| Email-Embedding | ~2-5 Sekunden | ~2-5 Sekunden | ~2-5 Sekunden |
| Base-Analyse | ~5-10 Minuten | ~15-20 Minuten | ~45+ Minuten |
| Optimize-Analyse | ~15-20 Minuten | ~40-60 Minuten | ~2+ Stunden |
| **50 Emails verarbeiten** | **~8+ Stunden** | **~24+ Stunden** | **~40+ Stunden** |

**CPU-Only Empfehlung:**
### Lokaler Betrieb mit dedizierter GPU (CUDA)
1. **Embedding:** `all-minilm:22m` ‚Äì Klein, schnell, ausreichend
2. **Base:** `llama3.2:3b` oder `llama3.1:8b` ‚Äì Gute Qualit√§t, schnell genug
3. **Optimize:** `llama3.1:8b` oder gr√∂√üer ‚Äì Maximale Qualit√§t

### Lokaler Betrieb CPU-only
1. **Embedding:** `all-minilm:22m` ‚Äì Klein, schnell, ausreichend (Learning macht den Unterschied!)
2. **Base:** `llama3.2:1b` ‚Äì **Einzige praktikable Option** f√ºr Batch-Processing
3. **Optimize:** `llama3.2:3b` oder **Cloud** ‚Äì Qualit√§t bei manueller Nutzung
4. **üí° Strategie:** Learning-System intensiv nutzen (3-5 Tags/Rejects pro Tag)

### Hybrid (Beste Balance)
1. **Embedding:** `all-minilm:22m` ‚Äì Lokal, keine Daten in Cloud
2. **Base:** `gpt-4o-mini` ‚Äì Cloud, schnell, g√ºnstig
3. **Optimize:** `claude-3-5-sonnet` ‚Äì Cloud, beste Qualit√§t

**Die wichtigste Erkenntnis:** 
- **Mit GPU:** Gr√∂√üere Modelle nutzen f√ºr bessere Out-of-the-Box-Qualit√§t
- **Ohne GPU:** Kleine Modelle + intensives Learning = Exzellente Ergebnisse
- **Negative Feedback (Phase F.3)** ist ein Game-Changer f√ºr schwache Hardware
**Warum CPU-only + Learning funktioniert:**
- Initial: Schwache Vorschl√§ge (15-25% Similarity)
- Nach 1 Woche (15-20 Tags): Gute Vorschl√§ge (75-85%)
- Nach 2 Wochen (30-40 Tags + 20-30 Rejects): Exzellente Vorschl√§ge (90-95%)
- System lernt deine Pr√§ferenzen, Modellgr√∂√üe wird weniger wichtig

## Performance-Richtwerte

### Mit GPU (RTX 3060 oder besser)

| Operation | Zeit |
|-----------|------|
| Email-Embedding | ~50 ms |
| Base-Analyse | <1 Sekunde |
| Optimize-Analyse | ~2-3 Sekunden |
| **50 Emails verarbeiten** | **~2-3 Minuten** |

### Ohne GPU (nur CPU)

| Operation | Zeit |
|-----------|------|
| Email-Embedding | ~2-5 Sekunden |
| Base-Analyse | ~5-10 Minuten |
| Optimize-Analyse | ~15-20 Minuten |
| **50 Emails verarbeiten** | **~8+ Stunden** |

**Empfehlung f√ºr CPU-Only:** Cloud-Provider f√ºr Base/Optimize nutzen, Embedding lokal.

---

## Zusammenfassung

1. **Embedding:** `all-minilm:22m` ‚Äì Klein, schnell, ausreichend (Learning macht den Unterschied)
2. **Base:** `llama3.2:1b` ‚Äì Schnell genug f√ºr Batch-Processing
3. **Optimize:** `llama3.2:3b` oder Cloud ‚Äì Qualit√§t bei manueller Nutzung

**Die wichtigste Erkenntnis:** Ein kleines Modell mit gutem Learning schl√§gt ein gro√ües Modell ohne Learning.
